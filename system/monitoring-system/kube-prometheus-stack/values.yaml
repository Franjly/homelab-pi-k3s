kube-prometheus-stack:
  ## Install Prometheus Operator CRDs
  ##
  crds:
    enabled: true

  ## Create default rules for monitoring the cluster
  ##
  defaultRules:
    create: true
    rules:
      alertmanager: true
      etcd: true
      configReloaders: true
      general: true
      k8sContainerCpuUsageSecondsTotal: true
      k8sContainerMemoryCache: true
      k8sContainerMemoryRss: true
      k8sContainerMemorySwap: true
      k8sContainerResource: true
      k8sContainerMemoryWorkingSetBytes: true
      k8sPodOwner: true
      kubeApiserverAvailability: true
      kubeApiserverBurnrate: true
      kubeApiserverHistogram: true
      kubeApiserverSlos: true
      kubeControllerManager: true
      kubelet: true
      kubeProxy: true
      kubePrometheusGeneral: true
      kubePrometheusNodeRecording: true
      kubernetesApps: true
      kubernetesResources: true
      kubernetesStorage: true
      kubernetesSystem: true
      kubeSchedulerAlerting: true
      kubeSchedulerRecording: true
      kubeStateMetrics: true
      network: true
      node: true
      nodeExporterAlerting: true
      nodeExporterRecording: true
      prometheus: true
      prometheusOperator: true
      windows: true

    ## Prefix for runbook URLs. Use this to override the first part of the runbookURLs that is common to all rules.
    runbookUrl: "https://runbooks.prometheus-operator.dev/runbooks"

    ## Disabled PrometheusRule alerts
    disabled: {}
    # KubeAPIDown: true
    # NodeRAIDDegraded: true

  ## Deprecated way to provide custom recording or alerting rules to be deployed into the cluster.
  ##
  # additionalPrometheusRules: []
  #  - name: my-rule-file
  #    groups:
  #      - name: my_group
  #        rules:
  #        - record: my_record
  #          expr: 100 * my_record

  ## Provide custom recording or alerting rules to be deployed into the cluster.
  ##
  additionalPrometheusRulesMap:
    ## Custom rules for TrueNAS
    rule-name:
      groups:
      - name: truenas-netdata
        rules:
          - alert: node_high_cpu_usage_70
            expr: sum(sum_over_time(netdata_system_cpu_percentage_average{dimension=~"(user|system|softirq|irq|guest)"}[10m])) by (job) / sum(count_over_time(netdata_system_cpu_percentage_average{dimension="idle"}[10m])) by (job) > 70
            for: 1m
            annotations:
              description: '{{ $labels.job }} on ''{{ $labels.job }}'' CPU usage is at {{ humanize $value }}%.'
              summary: CPU alert for container node '{{ $labels.job }}'
          - alert: node_high_memory_usage_70
            expr: 100 / sum(netdata_system_ram_MB_average) by (job) * sum(netdata_system_ram_MB_average{dimension=~"free|cached"}) by (job) < 30
            for: 1m
            annotations:
              description: '{{ $labels.job }} memory usage is {{ humanize $value}}%.'
              summary: Memory alert for container node '{{ $labels.job }}'
          - alert: node_low_root_filesystem_space_20
            expr: 100 / sum(netdata_disk_space_GB_average{family="/"}) by (job) * sum(netdata_disk_space_GB_average{family="/",dimension=~"avail|cached"}) by (job) < 20
            for: 1m
            annotations:
              description: '{{ $labels.job }} root filesystem space is {{ humanize $value}}%.'
              summary: Root filesystem alert for container node '{{ $labels.job }}'
          - alert: node_root_filesystem_fill_rate_6h
            expr: predict_linear(netdata_disk_space_GB_average{family="/",dimension=~"avail|cached"}[1h], 6 * 3600) < 0
            for: 1h
            labels:
              severity: critical
            annotations:
              description: Container node {{ $labels.job }} root filesystem is going to fill up in 6h.
              summary: Disk fill alert for Swarm node '{{ $labels.job }}'

  ##
  global:
    rbac:
      create: true

  ## Configuration for prometheus-windows-exporter
  ## ref: https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-windows-exporter
  ##
  prometheus-windows-exporter:
    ## Enable ServiceMonitor and set Kubernetes label to use as a job label
    ##
    prometheus:
      monitor:
        enabled: false

    releaseLabel: false

  ## Configuration for alertmanager
  ## ref: https://prometheus.io/docs/alerting/alertmanager/
  ##
  alertmanager:

    ## Deploy alertmanager
    ##
    enabled: true

    ## @param alertmanager.enableFeatures Enable access to Alertmanager disabled features.
    ##
    enableFeatures: []

    ## Service account for Alertmanager to use.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
    ##
    serviceAccount:
      create: true
      name: ""

    ## Alertmanager configuration directives
    ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file
    ##      https://prometheus.io/webtools/alerting/routing-tree-editor/
    ##
    config:
      global:
        resolve_timeout: 5m
      inhibit_rules:
        - source_matchers:
            - 'severity = critical'
          target_matchers:
            - 'severity =~ warning|info'
          equal:
            - 'namespace'
            - 'alertname'
        - source_matchers:
            - 'severity = warning'
          target_matchers:
            - 'severity = info'
          equal:
            - 'namespace'
            - 'alertname'
        - source_matchers:
            - 'alertname = InfoInhibitor'
          target_matchers:
            - 'severity = info'
          equal:
            - 'namespace'
        - target_matchers:
            - 'alertname = InfoInhibitor'
      route:
        group_by: ['namespace']
        group_wait: 30s
        group_interval: 5m
        repeat_interval: 12h
        receiver: 'discord_webhook'
        routes:
        - receiver: 'null'
          matchers:
            - alertname = "Watchdog"
      receivers:
      - name: 'null'
      - name: 'discord_webhook'
        webhook_configs:
        - url: 'http://alertmanager-discord.monitoring-system.svc.cluster.local:9094'
      templates:
      - '/etc/alertmanager/config/*.tmpl'

    ## Alertmanager configuration directives (as string type, preferred over the config hash map)
    ## stringConfig will be used only, if tplConfig is true
    ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file
    ##      https://prometheus.io/webtools/alerting/routing-tree-editor/
    ##
    stringConfig: ""

    ## Pass the Alertmanager configuration directives through Helm's templating
    ## engine. If the Alertmanager configuration contains Alertmanager templates,
    ## they'll need to be properly escaped so that they are not interpreted by
    ## Helm
    ## ref: https://helm.sh/docs/developing_charts/#using-the-tpl-function
    ##      https://prometheus.io/docs/alerting/configuration/#tmpl_string
    ##      https://prometheus.io/docs/alerting/notifications/
    ##      https://prometheus.io/docs/alerting/notification_examples/
    tplConfig: false

    ## Alertmanager template files to format alerts
    ## By default, templateFiles are placed in /etc/alertmanager/config/ and if
    ## they have a .tmpl file suffix will be loaded. See config.templates above
    ## to change, add other suffixes. If adding other suffixes, be sure to update
    ## config.templates above to include those suffixes.
    ## ref: https://prometheus.io/docs/alerting/notifications/
    ##      https://prometheus.io/docs/alerting/notification_examples/
    ##
    templateFiles: {}
    #
    ## An example template:
    #   template_1.tmpl: |-
    #       {{ define "cluster" }}{{ .ExternalURL | reReplaceAll ".*alertmanager\\.(.*)" "$1" }}{{ end }}
    #
    #       {{ define "slack.myorg.text" }}
    #       {{- $root := . -}}
    #       {{ range .Alerts }}
    #         *Alert:* {{ .Annotations.summary }} - `{{ .Labels.severity }}`
    #         *Cluster:* {{ template "cluster" $root }}
    #         *Description:* {{ .Annotations.description }}
    #         *Graph:* <{{ .GeneratorURL }}|:chart_with_upwards_trend:>
    #         *Runbook:* <{{ .Annotations.runbook }}|:spiral_note_pad:>
    #         *Details:*
    #           {{ range .Labels.SortedPairs }} - *{{ .Name }}:* `{{ .Value }}`
    #           {{ end }}
    #       {{ end }}
    #       {{ end }}

    ingress:
      enabled: true

      # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
      # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
      ingressClassName: nginx

      annotations:
        cert-manager.io/cluster-issuer: letsencrypt-staging
        # kubernetes.io/tls-acme: "true"
        hajimari.io/appName: AlertManager
        hajimari.io/icon: comment-alert
        hajimari.io/group: Monitoring
        hajimari.io/targetBlank: 'false'

      ## Hosts must be provided if Ingress is enabled.
      ##
      hosts:
        - alertmanager.local.tecno-fly.com

      ## Paths to use for ingress rules - one path should match the alertmanagerSpec.routePrefix
      ##
      paths:
      - /

      ## TLS configuration for Alertmanager Ingress
      ## Secret must be manually created in the namespace
      ##
      tls:
        - secretName: alertmanager-tls
          hosts:
          - alertmanager.local.tecno-fly.com

    ## Configuration for creating a ServiceMonitor for AlertManager
    ##
    serviceMonitor:
      ## If true, a ServiceMonitor will be created for the AlertManager service.
      ##
      selfMonitor: true

    ## Settings affecting alertmanagerSpec
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#alertmanagerspec
    ##
    alertmanagerSpec:

      ## Define Log Format
      # Use logfmt (default) or json logging
      logFormat: logfmt

      ## Log level for Alertmanager to be configured with.
      ##
      logLevel: info

      ## Size is the expected size of the alertmanager cluster. The controller will eventually make the size of the
      ## running cluster equal to the expected size.
      replicas: 1

      ## Time duration Alertmanager shall retain data for. Default is '120h', and must match the regular expression
      ## [0-9]+(ms|s|m|h) (milliseconds seconds minutes hours).
      ##
      retention: 120h

      ## Storage is the definition of how storage will be used by the Alertmanager instances.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md
      ##
      storage:
        volumeClaimTemplate:
          spec:
            storageClassName: longhorn
            resources:
              requests:
                storage: 5Gi

      ## Define which Nodes the Pods are scheduled on.
      ## ref: https://kubernetes.io/docs/user-guide/node-selection/
      ##
      nodeSelector: {}

  ## Using default values from https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml
  ##
  grafana:
    enabled: true

    ## ForceDeployDatasources Create datasource configmap even if grafana deployment has been disabled
    ##
    forceDeployDatasources: false

    ## ForceDeployDashboard Create dashboard configmap even if grafana deployment has been disabled
    ##
    forceDeployDashboards: false

    ## Deploy default dashboards
    ##
    defaultDashboardsEnabled: true

    ## Timezone for the default dashboards
    ## Other options are: browser or a specific timezone, i.e. Europe/Luxembourg
    ##
    defaultDashboardsTimezone: Europe/Madrid

    ## Editable flag for the default dashboards
    ##
    defaultDashboardsEditable: false

    adminUser: <path:pi-k3s/data/system/monitoring-system/grafana#admin-user>
    adminPassword: <path:pi-k3s/data/system/monitoring-system/grafana#admin-passwd>

    ingress:
      ## If true, Grafana Ingress will be created
      ##
      enabled: true

      ## IngressClassName for Grafana Ingress.
      ## Should be provided if Ingress is enable.
      ##
      ingressClassName: nginx

      ## Annotations for Grafana Ingress
      ##
      annotations:
        cert-manager.io/cluster-issuer: letsencrypt-staging
        # kubernetes.io/tls-acme: "true"
        hajimari.io/appName: Grafana
        hajimari.io/icon: chart-pie
        hajimari.io/group: Monitoring
        hajimari.io/targetBlank: 'false'

      ## Hostnames.
      ## Must be provided if Ingress is enable.
      ##
      hosts:
        - grafana.local.tecno-fly.com

      ## Path for grafana ingress
      path: /

      ## TLS configuration for grafana Ingress
      ## Secret must be manually created in the namespace
      ##
      tls:
        - secretName: grafana-tls
          hosts:
          - grafana.local.tecno-fly.com

    # # To make Grafana persistent (Using Statefulset)
    # #
    persistence:
      enabled: false
      type: sts
      storageClassName: longhorn
      accessModes:
        - ReadWriteOnce
      size: 5Gi
      finalizers:
        - kubernetes.io/pvc-protection

    serviceAccount:
      create: true
      autoMount: true

    ## Grafana plugins
    ##
    plugins:
      - grafana-piechart-panel
      - grafana-clock-panel

    sidecar:
      dashboards:
        enabled: true
        label: grafana_dashboard
        labelValue: "1"
        folder: /tmp/dashboards
        # Allow discovery in all namespaces for dashboards
        searchNamespace: ALL

        # Support for new table panels, when enabled grafana auto migrates the old table panels to newer table panels
        enableNewTablePanelSyntax: false

        ## Annotations for Grafana dashboard configmaps
        ##
        annotations:
          k8s-sidecar-target-directory: /tmp/dashboards/kubernetes
        
        provider:
          allowUiUpdates: false
          foldersFromFilesStructure: true
      datasources:
        enabled: true
        defaultDatasourceEnabled: true
        isDefaultDatasource: true

        name: Prometheus
        uid: prometheus

        ## URL of prometheus datasource
        ##
        # url: http://prometheus-stack-prometheus:9090/

        ## Create datasource for each Pod of Prometheus StatefulSet;
        ## this uses headless service `prometheus-operated` which is
        ## created by Prometheus Operator
        ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/0fee93e12dc7c2ea1218f19ae25ec6b893460590/pkg/prometheus/statefulset.go#L255-L286
        createPrometheusReplicasDatasources: false
        label: grafana_datasource
        labelValue: "1"

        ## Field with internal link pointing to existing data source in Grafana.
        ## Can be provisioned via additionalDataSources
        exemplarTraceIdDestinations: {}
          # datasourceUid: Jaeger
          # traceIdLabelName: trace_id
        alertmanager:
          enabled: true
          name: Alertmanager
          uid: alertmanager
          handleGrafanaManagedAlerts: false
          implementation: prometheus

    deleteDatasources: []
    # - name: example-datasource
    #   orgId: 1

    ## Configure additional grafana datasources (passed through tpl)
    ## ref: http://docs.grafana.org/administration/provisioning/#datasources
    additionalDataSources:
      - name: Loki
        type: loki
        uid: loki
        url: http://loki.monitoring-system:3100
        isDefault: false

    ## Configure grafana dashboard providers
    ## ref: http://docs.grafana.org/administration/provisioning/#dashboards
    ##
    ## `path` must be /var/lib/grafana/dashboards/<provider_name>
    ##
    dashboardProviders: 
      dashboardproviders.yaml:
        apiVersion: 1
        providers:
        - name: 'loki'
          orgId: 1
          folder: 'Loki'
          type: file
          disableDeletion: true
          editable: true
          options:
            path: /var/lib/grafana/dashboards/loki
        - name: 'unifi-poller'
          orgId: 1
          folder: 'UniFi-Poller'
          type: file
          disableDeletion: true
          editable: true
          options:
            path: /var/lib/grafana/dashboards/unifi-poller
        - name: 'ingress-nginx'
          orgId: 1
          folder: 'NGINX Ingress'
          type: file
          disableDeletion: true
          editable: true
          options:
            path: /var/lib/grafana/dashboards/ingress-nginx
        - name: 'pi-hole'
          orgId: 1
          folder: 'Pi-hole'
          type: file
          disableDeletion: true
          editable: true
          options:
            path: /var/lib/grafana/dashboards/pi-hole
        - name: 'calico'
          orgId: 1
          folder: 'Calico'
          type: file
          disableDeletion: true
          editable: true
          options:
            path: /var/lib/grafana/dashboards/calico
        - name: 'cert-manager'
          orgId: 1
          folder: 'Cert-Manager'
          type: file
          disableDeletion: true
          editable: true
          options:
            path: /var/lib/grafana/dashboards/cert-manager
        - name: 'longhorn'
          orgId: 1
          folder: 'Longhorn'
          type: file
          disableDeletion: true
          editable: true
          options:
            path: /var/lib/grafana/dashboards/longhorn
        - name: 'velero'
          orgId: 1
          folder: 'Velero'
          type: file
          disableDeletion: true
          editable: true
          options:
            path: /var/lib/grafana/dashboards/velero
        - name: 'argocd'
          orgId: 1
          folder: 'ArgoCD'
          type: file
          disableDeletion: true
          editable: true
          options:
            path: /var/lib/grafana/dashboards/argocd
        - name: 'truenas'
          orgId: 1
          folder: 'TrueNAS'
          type: file
          disableDeletion: true
          editable: true
          options:
            path: /var/lib/grafana/dashboards/truenas

    ## Configure grafana dashboard to import
    ## NOTE: To use dashboards you must also enable/configure dashboardProviders
    ## ref: https://grafana.com/dashboards
    ##
    ## dashboards per provider, use provider name as key.
    ##
    dashboards:
      loki:
        loki-dashboard:
          gnetId: 10880
          revision: 1
          datasource: prometheus
        loki-logging-dashboard:
          url: https://raw.githubusercontent.com/Franjly/grafana-dashboards-pi-k3s/main/loki/loki-dashboard.json
          datasource: prometheus
        loki-another-logging-dashboard:
          gnetId: 16966
          revision: 1
          datasource: prometheus
      unifi-poller:
        USG-Insights:
          gnetId: 11313
          revision: 9
          datasource: prometheus
        USW-Insights:
          gnetId: 11312
          revision: 9
          datasource: prometheus
        UAP-Insights:
          gnetId: 11314
          revision: 10
          datasource: prometheus
        Client-Insights:
          gnetId: 11315
          revision: 9
          datasource: prometheus
        Network-Sites:
          gnetId: 11311
          revision: 5
          datasource: prometheus
        Client-DPI:
          gnetId: 11310
          revision: 5
          datasource: prometheus
      ingress-nginx:
        Kubernetes-Nginx-Ingress-Controller:
          gnetId: 14314
          revision: 2
          datasource: prometheus
        NGINX-Ingress-Controller:
          url: https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/grafana/dashboards/nginx.json
          datasource: prometheus
        Request-Handling-Performance:
          url: https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/grafana/dashboards/request-handling-performance.json
          datasource: prometheus
      pi-hole:
        PI-Hole-Exporter:
          url: https://raw.githubusercontent.com/eko/pihole-exporter/master/grafana/dashboard.json
          datasource: prometheus
      calico:
        Felix-Dashboard:
          gnetId: 12175
          revision: 5
          datasource: prometheus
        Kubernetes-Calico:
          url: https://raw.githubusercontent.com/Franjly/grafana-dashboards-pi-k3s/main/calico/kubernetes-calico.json
          datasource: prometheus
      cert-manager:
        cert-manager-dashboard:
          url: https://raw.githubusercontent.com/Franjly/grafana-dashboards-pi-k3s/main/cert-manager/cert-manager-dashboard.json
          datasource: prometheus
      velero:
        velero-dashboard:
          url: https://raw.githubusercontent.com/Franjly/grafana-dashboards-pi-k3s/main/velero/velero-dashboard.json
          datasource: prometheus
      longhorn:
        longhorn-dashboard:
          url: https://raw.githubusercontent.com/Franjly/grafana-dashboards-pi-k3s/main/longhorn/longhorn-dashboard.json
          datasource: prometheus
      argocd:
        argocd-dashboard:
          url: https://raw.githubusercontent.com/Franjly/grafana-dashboards-pi-k3s/main/argocd/argocd-dashboard.json
          datasource: prometheus
      truenas:
        truenas-netdata:
          url: https://raw.githubusercontent.com/Franjly/grafana-dashboards-pi-k3s/main/truenas/netdata.json
          datasource: prometheus

    # Flag to mark provisioned data sources for deletion if they are no longer configured.
    # It takes no effect if data sources are already listed in the deleteDatasources section.
    # ref: https://grafana.com/docs/grafana/latest/administration/provisioning/#example-data-source-config-file
    prune: false

    serviceMonitor:
      # If true, a ServiceMonitor CRD is created for a prometheus operator
      # https://github.com/coreos/prometheus-operator
      #
      enabled: true

  ## Flag to disable all the kubernetes component scrapers
  ##
  kubernetesServiceMonitors:
    enabled: true

  ## Component scraping the kube api server
  ##
  kubeApiServer:
    enabled: true
    tlsConfig:
      serverName: kubernetes
      insecureSkipVerify: false
    serviceMonitor:
      jobLabel: component
      selector:
        matchLabels:
          component: apiserver
          provider: kubernetes

  ## Component scraping the kubelet and kubelet-hosted cAdvisor
  ##
  kubelet:
    enabled: true
    namespace: kube-system

    serviceMonitor:
      ## Attach metadata to discovered targets. Requires Prometheus v2.45 for endpoints created by the operator.
      ##
      attachMetadata:
        node: false

      ## Enable scraping the kubelet over https. For requirements to enable this see
      ## https://github.com/prometheus-operator/prometheus-operator/issues/926
      ##
      https: true

      ## Skip TLS certificate validation when scraping.
      ## This is enabled by default because kubelet serving certificate deployed by kubeadm is by default self-signed
      ## ref: https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#kubelet-serving-certs
      ##
      insecureSkipVerify: true

  ## Component scraping the kube controller manager
  ##
  kubeControllerManager:
    enabled: true

    ## If your kube controller manager is not deployed as a pod, specify IPs it can be found on
    ##
    endpoints:
      - 192.168.30.10

    ## If using kubeControllerManager.endpoints only the port and targetPort are used
    ##
    service:
      enabled: true
      # selector:
      #   component: kube-controller-manager

    serviceMonitor:
      enabled: true

      jobLabel: jobLabel
      selector: {}
      #  matchLabels:
      #    component: kube-controller-manager

      ## Enable scraping kube-controller-manager over https.
      ## Requires proper certs (not self-signed) and delegated authentication/authorization checks.
      ## If null or unset, the value is determined dynamically based on target Kubernetes version.
      ##
      https: null

      # Skip TLS certificate validation when scraping
      insecureSkipVerify: null

      # Name of the server to use when validating TLS certificate
      serverName: null

  ## Component scraping coreDns. Use either this or kubeDns
  ##
  coreDns:
    enabled: true
    service:
      enabled: true
      # selector:
      #   k8s-app: kube-dns

    serviceMonitor:
      enabled: true

      jobLabel: jobLabel
      selector: {}
      #  matchLabels:
      #    k8s-app: kube-dns

  ## Component scraping kubeDns. Use either this or coreDns
  ##
  kubeDns:
    enabled: false

  ## Component scraping etcd
  ##
  kubeEtcd:
    enabled: false

    ## If your etcd is not deployed as a pod, specify IPs it can be found on
    ##
    endpoints:
      - 192.168.30.10

    ## Etcd service. If using kubeEtcd.endpoints only the port and targetPort are used
    ##
    service:
      enabled: true
      # selector:
      #   component: etcd

    ## Configure secure access to the etcd cluster by loading a secret into prometheus and
    ## specifying security configuration below. For example, with a secret named etcd-client-cert
    ##
    ## serviceMonitor:
    ##   scheme: https
    ##   insecureSkipVerify: false
    ##   serverName: localhost
    ##   caFile: /etc/prometheus/secrets/etcd-client-cert/etcd-ca
    ##   certFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client
    ##   keyFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client-key
    ##
    serviceMonitor:
      enabled: true

      ## proxyUrl: URL of a proxy that should be used for scraping.
      ##
      proxyUrl: ""
      scheme: http
      insecureSkipVerify: false
      serverName: ""
      caFile: ""
      certFile: ""
      keyFile: ""

      jobLabel: jobLabel
      # selector:
      #  matchLabels:
      #    component: etcd

  ## Component scraping kube scheduler
  ##
  kubeScheduler:
    enabled: true

    ## If your kube scheduler is not deployed as a pod, specify IPs it can be found on
    ##
    endpoints:
      - 192.168.30.10

    ## If using kubeScheduler.endpoints only the port and targetPort are used
    ##
    service:
      enabled: true
      # selector:
      #   component: kube-scheduler

    serviceMonitor:
      enabled: true

      jobLabel: jobLabel
      selector: {}
      #  matchLabels:
      #    component: kube-scheduler

      ## Skip TLS certificate validation when scraping
      insecureSkipVerify: null

  ## Component scraping kube proxy
  ##
  kubeProxy:
    enabled: true

    ## If your kube proxy is not deployed as a pod, specify IPs it can be found on
    ##
    endpoints:
      - 192.168.30.10
      - 192.168.30.20
      - 192.168.30.21

    service:
      enabled: true
      # selector:
      #   k8s-app: kube-proxy

    serviceMonitor:
      enabled: true

      jobLabel: jobLabel
      selector: {}
      #  matchLabels:
      #    k8s-app: kube-proxy

      ## Enable scraping kube-proxy over https.
      ## Requires proper certs (not self-signed) and delegated authentication/authorization checks
      ##
      https: false

  ## Component scraping kube state metrics
  ##
  kubeStateMetrics:
    enabled: true

  ## Configuration for kube-state-metrics subchart
  ##
  kube-state-metrics:
    rbac:
      create: true

    releaseLabel: true

    prometheus:
      monitor:
        enabled: true

    selfMonitor:
      enabled: false

  ## Deploy node exporter as a daemonset to all nodes
  ##
  nodeExporter:
    enabled: true
    operatingSystems:
      linux:
        enabled: true
      darwin:
        enabled: true

    ## ForceDeployDashboard Create dashboard configmap even if nodeExporter deployment has been disabled
    ##
    forceDeployDashboards: false

  ## Configuration for prometheus-node-exporter subchart
  ##
  prometheus-node-exporter:

    podLabels:
      ## Add the 'node-exporter' label to be used by serviceMonitor to match standard common usage in rules and grafana dashboards
      ##
      jobLabel: node-exporter
    
    releaseLabel: true
    
    extraArgs:
      - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
      - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
    
    service:
      portName: http-metrics
      
    prometheus:
      monitor:
        enabled: true

  ## Manages Prometheus and Alertmanager components
  ##
  prometheusOperator:
    enabled: true

    ## Prometheus-Operator v0.39.0 and later support TLS natively.
    ##
    tls:
      enabled: true
      # Value must match version names from https://golang.org/pkg/crypto/tls/#pkg-constants
      tlsMinVersion: VersionTLS13
      # The default webhook port is 10250 in order to work out-of-the-box in GKE private clusters and avoid adding firewall rules.
      internalPort: 10250

    ## Namespaces to scope the interaction of the Prometheus Operator and the apiserver (allow list).
    ## This is mutually exclusive with denyNamespaces. Setting this to an empty object will disable the configuration
    ##
    namespaces: {}
      # releaseNamespace: true
      # additional:
      # - kube-system

    ## Namespaces not to scope the interaction of the Prometheus Operator (deny list).
    ##
    denyNamespaces: []

    ## Filter namespaces to look for prometheus-operator custom resources
    ##
    alertmanagerInstanceNamespaces: []
    alertmanagerConfigNamespaces: []
    prometheusInstanceNamespaces: []
    thanosRulerInstanceNamespaces: []

    ## The clusterDomain value will be added to the cluster.peer option of the alertmanager.
    ## Without this specified option cluster.peer will have value alertmanager-monitoring-alertmanager-0.alertmanager-operated:9094 (default value)
    ## With this specified option cluster.peer will have value alertmanager-monitoring-alertmanager-0.alertmanager-operated.namespace.svc.cluster-domain:9094
    ##
    # clusterDomain: "cluster.local"

    networkPolicy:
      ## Enable creation of NetworkPolicy resources.
      ##
      enabled: false

      ## Flavor of the network policy to use.
      #  Can be:
      #  * kubernetes for networking.k8s.io/v1/NetworkPolicy
      #  * cilium     for cilium.io/v2/CiliumNetworkPolicy
      flavor: kubernetes

      # cilium:
      #   egress:

      ## match labels used in selector
      # matchLabels: {}

    ## Service account for Prometheus Operator to use.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
    ##
    serviceAccount:
      create: true
      name: ""
      automountServiceAccountToken: true

    kubeletService:
      ## If true, the operator will create and maintain a service for scraping kubelets
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/helm/prometheus-operator/README.md
      ##
      enabled: true
      namespace: kube-system
      selector: ""
      ## Use '{{ template "kube-prometheus-stack.fullname" . }}-kubelet' by default
      name: ""

    ## Create a servicemonitor for the operator
    ##
    serviceMonitor:
      ## If true, create a serviceMonitor for prometheus operator
      ##
      selfMonitor: true

    ## Operator Environment
    ##  env:
    ##    VARIABLE: value
    env:
      GOGC: "30"

    ## Define which Nodes the Pods are scheduled on.
    ## ref: https://kubernetes.io/docs/user-guide/node-selection/
    ##
    nodeSelector: {}

  ## Deploy a Prometheus instance
  ##
  prometheus:
    enabled: true

    ## Configure network policy for the prometheus
    networkPolicy:
      enabled: false

      ## Flavor of the network policy to use.
      #  Can be:
      #  * kubernetes for networking.k8s.io/v1/NetworkPolicy
      #  * cilium     for cilium.io/v2/CiliumNetworkPolicy
      flavor: kubernetes

      # cilium:
      #   endpointSelector:
      #   egress:
      #   ingress:

      # egress:
      # - {}
      # ingress:
      # - {}
      # podSelector:
      #   matchLabels:
      #     app: prometheus

    ## Service account for Prometheuses to use.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
    ##
    serviceAccount:
      create: true

    ## ExtraSecret can be used to store various data in an extra secret
    ## (use it for example to store hashed basic auth credentials)
    extraSecret:
      ## if not set, name will be auto generated
      # name: ""
      annotations: {}
      data: {}
    #   auth: |
    #     foo:$apr1$OFG3Xybp$ckL0FHDAkoXYIlH9.cysT0
    #     someoneelse:$apr1$DMZX2Z4q$6SbQIfyuLQd.xmo/P0m2c.

    ingress:
      enabled: true

      # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
      # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
      ingressClassName: nginx

      annotations:
        cert-manager.io/cluster-issuer: letsencrypt-staging
        # kubernetes.io/tls-acme: "true"
        hajimari.io/appName: Prometheus
        hajimari.io/icon: chart-areaspline
        hajimari.io/group: Monitoring
        hajimari.io/targetBlank: 'false'

      ## Hostnames.
      ## Must be provided if Ingress is enabled.
      ##
      hosts:
        - prometheus.local.tecno-fly.com

      ## Paths to use for ingress rules - one path should match the prometheusSpec.routePrefix
      ##
      paths:
        - /

      ## TLS configuration for Prometheus Ingress
      ## Secret must be manually created in the namespace
      ##
      tls:
        - secretName: prometheus-tls
          hosts:
            - prometheus.local.tecno-fly.com

    serviceMonitor:
      ## If true, create a serviceMonitor for prometheus
      ##
      selfMonitor: true

    ## Settings affecting prometheusSpec
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#prometheusspec
    ##
    prometheusSpec:
      ## EnableAdminAPI enables Prometheus the administrative HTTP API which includes functionality such as deleting time series.
      ## This is disabled by default.
      ## ref: https://prometheus.io/docs/prometheus/latest/querying/api/#tsdb-admin-apis
      ##
      enableAdminAPI: false

      # EnableFeatures API enables access to Prometheus disabled features.
      # ref: https://prometheus.io/docs/prometheus/latest/disabled_features/
      enableFeatures: []
      # - exemplar-storage

      ## Define which Nodes the Pods are scheduled on.
      ## ref: https://kubernetes.io/docs/user-guide/node-selection/
      ##
      nodeSelector: {}

      ## If nil, select own namespace. Namespaces to be selected for PrometheusRules discovery.
      ruleNamespaceSelector: {}
      ## Example which selects PrometheusRules in namespaces with label "prometheus" set to "somelabel"
      # ruleNamespaceSelector:
      #   matchLabels:
      #     prometheus: somelabel

      ## If true, a nil or {} value for prometheus.prometheusSpec.ruleSelector will cause the
      ## prometheus resource to be created with selectors based on values in the helm deployment,
      ## which will also match the PrometheusRule resources created
      ##
      ruleSelectorNilUsesHelmValues: false

      ## PrometheusRules to be selected for target discovery.
      ## If {}, select all PrometheusRules
      ##
      ruleSelector: {}
      ## Example which select all PrometheusRules resources
      ## with label "prometheus" with values any of "example-rules" or "example-rules-2"
      # ruleSelector:
      #   matchExpressions:
      #     - key: prometheus
      #       operator: In
      #       values:
      #         - example-rules
      #         - example-rules-2
      #
      ## Example which select all PrometheusRules resources with label "role" set to "example-rules"
      # ruleSelector:
      #   matchLabels:
      #     role: example-rules

      ## If true, a nil or {} value for prometheus.prometheusSpec.serviceMonitorSelector will cause the
      ## prometheus resource to be created with selectors based on values in the helm deployment,
      ## which will also match the servicemonitors created
      ##
      serviceMonitorSelectorNilUsesHelmValues: false

      ## ServiceMonitors to be selected for target discovery.
      ## If {}, select all ServiceMonitors
      ##
      serviceMonitorSelector: {}
      ## Example which selects ServiceMonitors with label "prometheus" set to "somelabel"
      # serviceMonitorSelector:
      #   matchLabels:
      #     prometheus: somelabel

      ## Namespaces to be selected for ServiceMonitor discovery.
      ##
      serviceMonitorNamespaceSelector: {}
      ## Example which selects ServiceMonitors in namespaces with label "prometheus" set to "somelabel"
      # serviceMonitorNamespaceSelector:
      #   matchLabels:
      #     prometheus: somelabel

      ## If true, a nil or {} value for prometheus.prometheusSpec.podMonitorSelector will cause the
      ## prometheus resource to be created with selectors based on values in the helm deployment,
      ## which will also match the podmonitors created
      ##
      podMonitorSelectorNilUsesHelmValues: false

      ## PodMonitors to be selected for target discovery.
      ## If {}, select all PodMonitors
      ##
      podMonitorSelector: {}
      ## Example which selects PodMonitors with label "prometheus" set to "somelabel"
      # podMonitorSelector:
      #   matchLabels:
      #     prometheus: somelabel

      ## If nil, select own namespace. Namespaces to be selected for PodMonitor discovery.
      podMonitorNamespaceSelector: {}
      ## Example which selects PodMonitor in namespaces with label "prometheus" set to "somelabel"
      # podMonitorNamespaceSelector:
      #   matchLabels:
      #     prometheus: somelabel

      ## If true, a nil or {} value for prometheus.prometheusSpec.probeSelector will cause the
      ## prometheus resource to be created with selectors based on values in the helm deployment,
      ## which will also match the probes created
      ##
      probeSelectorNilUsesHelmValues: false

      ## Probes to be selected for target discovery.
      ## If {}, select all Probes
      ##
      probeSelector: {}
      ## Example which selects Probes with label "prometheus" set to "somelabel"
      # probeSelector:
      #   matchLabels:
      #     prometheus: somelabel

      ## If nil, select own namespace. Namespaces to be selected for Probe discovery.
      probeNamespaceSelector: {}
      ## Example which selects Probe in namespaces with label "prometheus" set to "somelabel"
      # probeNamespaceSelector:
      #   matchLabels:
      #     prometheus: somelabel

      ## If true, a nil or {} value for prometheus.prometheusSpec.scrapeConfigSelector will cause the
      ## prometheus resource to be created with selectors based on values in the helm deployment,
      ## which will also match the scrapeConfigs created
      ##
      scrapeConfigSelectorNilUsesHelmValues: false

      ## scrapeConfigs to be selected for target discovery.
      ## If {}, select all scrapeConfigs
      ##
      scrapeConfigSelector: {}
      ## Example which selects scrapeConfigs with label "prometheus" set to "somelabel"
      # scrapeConfigSelector:
      #   matchLabels:
      #     prometheus: somelabel

      ## If nil, select own namespace. Namespaces to be selected for scrapeConfig discovery.
      scrapeConfigNamespaceSelector: {}
      ## Example which selects scrapeConfig in namespaces with label "prometheus" set to "somelabel"
      # scrapeConfigNamespaceSelector:
      #   matchLabels:
      #     prometheus: somelabel

      ## How long to retain metrics
      ##
      retention: 10d

      ## Maximum size of metrics
      ##
      retentionSize: ""

      ## Number of replicas of each shard to deploy for a Prometheus deployment.
      ## Number of replicas multiplied by shards is the total number of Pods created.
      ##
      replicas: 1

      ## Log level for Prometheus be configured in
      ##
      logLevel: info

      ## Log format for Prometheus be configured in
      ##
      logFormat: logfmt

      ## Prometheus StorageSpec for persistent data
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md
      ##
      storageSpec:
        volumeClaimTemplate:
          spec:
            storageClassName: nfs-prometheus
            resources:
              requests:
                storage: 150Gi

      ## AdditionalScrapeConfigs allows specifying additional Prometheus scrape configurations. Scrape configurations
      ## are appended to the configurations generated by the Prometheus Operator. Job configurations must have the form
      ## as specified in the official Prometheus documentation:
      ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config. As scrape configs are
      ## appended, the user is responsible to make sure it is valid. Note that using this feature may expose the possibility
      ## to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible
      ## scrape configs are going to break Prometheus after the upgrade.
      ## AdditionalScrapeConfigs can be defined as a list or as a templated string.
      ##
      ## The scrape configuration example below will find master nodes, provided they have the name .*mst.*, relabel the
      ## port to 2379 and allow etcd scraping provided it is running on all Kubernetes master nodes
      ##
      additionalScrapeConfigs:
        - job_name: 'pi-hole-0'
          scrape_interval: 1m
          metrics_path: '/metrics'
          params:
            format: [prometheus]
          honor_labels: true
          static_configs:
          - targets: ['192.168.40.254:9617']
        - job_name: 'truenas-netdata'
          scrape_interval: 1m
          metrics_path: '/api/v1/allmetrics'
          params:
            format: [prometheus]
          honor_labels: true
          static_configs:
          - targets: ['192.168.40.250:20489']
        - job_name: 'truenas-graphite'
          scrape_interval: 1m
          metrics_path: '/metrics'
          static_configs:
          - targets: ['192.168.40.250:9108']
        # - job_name: minio-job
        #   bearer_token: eyJhbGciOiJIUzUxMiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJwcm9tZXRoZXVzIiwic3ViIjoiczNhZG1pbiIsImV4cCI6NDg3NzUwOTM1OH0.PgciIlXf8uJ3EEo6Ns3pY8ofaI3hMlyiYCohu4G0DZZHj_GVvh8y4cfeUuA6scE7BNCF0GovHQOuRP3ckG-OFQ
        #   metrics_path: /minio/v2/metrics/cluster
        #   scheme: https
        #   tls_config:
        #     insecure_skip_verify: true
        #   static_configs:
        #   - targets: ['192.168.40.250:9000']

  ## Setting to true produces cleaner resource names, but requires a data migration because the name of the persistent volume changes. Therefore this should only be set once on initial installation.
  ##
  cleanPrometheusOperatorObjectNames: false

  ## Extra manifests to deploy as an array
  extraManifests: []
    # - apiVersion: v1
    #   kind: ConfigMap
    #   metadata:
    #   labels:
    #     name: prometheus-extra
    #   data:
    #     extra-data: "value"